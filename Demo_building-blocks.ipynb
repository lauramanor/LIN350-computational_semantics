{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Choose a corpus\n",
    "\n",
    "- More data is usually better because you get more stable estimates. But more data creates more processing headaches, both because of long processing times and large amounts of memory needed.\n",
    "- Genre matters! Dekang Lin, when computing a model only from newspaper data, got \"captive\" as the word that was overall most similar to \"westerner\", and vice versa. He also got \"Republican\" and \"terrorist\" as very close neighbors of \"adversary\" (though it is not clear whether they were found as synonyms or antonyms, as distributional models are not good at distinguishing those two.) \n",
    "\n",
    "For demonstration purposes, we choose a very short extract from Project Gutenberg, available [here](https://utexas.box.com/shared/static/aqwjnm50xj3wmn47mk3qkhr1yyn73loy.zip). If you would like to try a slightly bigger corpus, [try this version](https://utexas.box.com/shared/static/84ebvg8ajlbvgylr6bn0iwb8l0dj5uhd.zip). A much bigger collection of novels from Project Gutenberg is [here](https://utexas.box.com/shared/static/2n18p5cq98en2tcui3dahxhdc0n2h8qu.zip).\n",
    "\n",
    "Here is code that reads in one file of this collection at a time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace this directory with one on your own machine\n",
    "\n",
    "demo_dir = \"./gothic_novels\"\n",
    "\n",
    "import os\n",
    "\n",
    "# We iterate over the corpus files. \n",
    "\n",
    "# os.listdir lists the names of all files in a directory\n",
    "for filename in os.listdir(demo_dir):\n",
    "    if filename.endswith(\"txt\"):\n",
    "        #print(\"reading file\", filename)\n",
    "        text = open(os.path.join(demo_dir, filename)).read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Choose preprocessing steps\n",
    "- You probably want to lowercase all words.\n",
    "- Do you want to keep wordforms, or lemmatize them, or use a stemmer? Wordforms will have counts dispersed over different forms of the same word. Lemmatization is more accurate than stemming, but stemming is faster.\n",
    "- Do you want to apply part-of-speech tagging, for example to distinguish object-N and object-V? \n",
    "- Do you want to eliminate some words up front? Stopwords? Or maybe all words except a few class of content words, typically NN (nouns), JJ (adjectives), VB (verbs), RB (adverbs)?\n",
    "\n",
    "Here is code that splits a string into words and lowercases. You have done further preprocessing in homework 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this', 'is', 'a', 'test']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##\n",
    "# NLTK processing objects\n",
    "\n",
    "import nltk\n",
    "\n",
    "import string\n",
    "\n",
    "def preprocess(s):\n",
    "    # split up into words, lowercase, remove punctuation at beginning and end of word\n",
    "    return [ w.lower().strip(string.punctuation) for w in s.split() ]\n",
    "\n",
    "# or like this:\n",
    "# def preprocess(s):\n",
    "#     words =  [ ]\n",
    "#     for w in s.split():\n",
    "#         word = w.lower()\n",
    "#         word = word.strip(string.punctuation)\n",
    "#         words.append(word)\n",
    "#     return words\n",
    "\n",
    "\n",
    "# use the function like this:\n",
    "\n",
    "preprocess(\"This is a test.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Define a frequency cutoff\n",
    "Infrequent words are not useful for distributional models. An infrequent target word will have too low counts on all dimensions to get good similarity estimates. An infrequent context item will not contribute much to the representation of any targets. Also, the more different context items you count, the larger your table of counts. The table of counts can quickly become unwieldy, and using a frequency cutoff is a good way of keeping it manageable.\n",
    "\n",
    "We go through the whole corpus, and count the frequency of all words in it. Then we retain only the N most frequent words, for example N=2000 for our small demo corpus, or N=10,000 for a larger corpus. We use these words as both the list of target words and the list of context items. \n",
    "\n",
    "The following code does the counting and retains only the N most frequent words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing a frequency-based cutoff: keeping only the N most frequent context words.\n",
      "Keeping only 10 dimensions, then I get: ['the', 'and', 'of', 'i', 'to', 'a', 'my', 'in', 'was', 'that'] \n",
      "\n",
      "Keeping 100 dimensions, then I get: ['the', 'and', 'of', 'i', 'to', 'a', 'my', 'in', 'was', 'that', 'with', 'me', 'he', 'you', 'had', 'it', 'but', 'his', 'as', 'for', 'which', 'not', 'this', 'on', 'by', 'be', 'from', 'at', 'have', 'is', 'when', 'or', 'were', 'her', 'him', 'your', 'all', 'if', 'an', 'so', 'one', 'will', 'she', 'no', 'they', 'been', 'are', 'could', 'more', 'we', 'would', 'said', 'now', 'their', 'these', 'who', 'man', 'what', 'some', 'do', 'before', 'upon', 'should', 'yet', 'any', 'into', 'project', 'there', 'life', 'then', 'myself', 'only', 'its', 'am', 'than', 'them', 'may', 'own', 'did', 'our', 'even', 'can', 'other', 'mr', 'shall', 'might', 'very', 'every', 'first', 'time', 'work', 'utterson', 'up', 'must', 'how', 'eyes', 'saw', 'such', 'like', 'again'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#####################\n",
    "# Counting words:\n",
    "# We want to make a list of the N most frequent words in our corpus\n",
    "\n",
    "import os\n",
    "\n",
    "def do_word_count(demo_dir, numdims):\n",
    "    # we store the counts in word_count\n",
    "    # using NLTK's FreqDist\n",
    "    word_count = nltk.FreqDist()\n",
    "    \n",
    "    # We iterate over the corpus files\n",
    "    for filename in os.listdir(demo_dir):\n",
    "        if filename.endswith(\"txt\"):\n",
    "            #print(\"reading file\", filename)\n",
    "            text = open(os.path.join(demo_dir, filename)).read()\n",
    "            word_count.update(preprocess(text))\n",
    "            \n",
    "    # keep_wordfreq is a list of (word, frequency) pairs\n",
    "    keep_wordfreq = word_count.most_common(numdims)\n",
    "    keep_these_words = [ w for w, freq in keep_wordfreq ]\n",
    "    # print(\"Target words:\\n\", keep_these_words, \"\\n\")\n",
    "    \n",
    "    return keep_these_words\n",
    "\n",
    "# or like this, without FreqDist:\n",
    "# def do_word_count(demo_dir, numdims):\n",
    "#     word_count = { }\n",
    "\n",
    "#     for filename in os.listdir(demo_dir):\n",
    "#         if filename.endswith(\"txt\"):\n",
    "#             print(\"reading file\", filename)\n",
    "#         text = open(os.path.join(demo_dir, filename)).read()\n",
    "#         for taggedword in preprocess(text):\n",
    "#             if taggedword not in word_count:\n",
    "#                 word_count[ taggedword ] = 0\n",
    "#             word_count[ taggedword ] += 1\n",
    "#\n",
    "#     def map_word_to_count(word): return word_count[ word ]\n",
    "#     keep_these_words = sorted(word_count.keys(), key = map_word_to_count)[:numdims]\n",
    "#     \n",
    "#     # print(\"Target words (and also dimensions):\\n\", keep_these_words, \"\\n\")\n",
    "#\n",
    "#     return keep_these_words\n",
    "\n",
    "\n",
    "\n",
    "##\n",
    "# run this:\n",
    "def test_wordcount():\n",
    "    print(\"Doing a frequency-based cutoff: keeping only the N most frequent context words.\")\n",
    "    \n",
    "    # with 10 dimensions\n",
    "    keepwords = do_word_count(demo_dir, 10)\n",
    "    print(\"Keeping only 10 dimensions, then I get:\", keepwords, \"\\n\")\n",
    "\n",
    "    # with 100 dimensions\n",
    "    keepwords = do_word_count(demo_dir, 100)\n",
    "    print(\"Keeping 100 dimensions, then I get:\", keepwords, \"\\n\")\n",
    "    \n",
    "test_wordcount()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Choose a context window\n",
    "We want to make a table of counts for each target word. This table of counts will show how often each context item co-occurred with the target word. But what does it mean to co-occur? \n",
    "\n",
    "- A \"narrow context window\" is one that counts 2 or 3 words on either side of the target, or even all words in the sentence where the target occurs. If you count 2 or 3 words on either side of the target: Do you want to cross sentence boundaries?\n",
    "- A \"wide context window\" is one that counts 20 or 50 words on either side of the target, ignoring sentence boundaries, or maybe all words that occur in the same document as the target. (That only makes sense if you have lots of documents.)\n",
    "- If you have a corpus that has been analyzed for sentence structure by a syntactic parser, you can also say that your context window is all the parse tree snippets that link directly to your target word in the parse of the sentence. (But we don't do that here.)\n",
    "\n",
    "Here is a function that counts co-occurrences in a window of 2 words on either side of the target. It takes as input a sequence of words (supposed to be a single sentence), and returns a list of pairs (word1, word2) where word1 is a target and word2 is a context item that co-occurrence with the target in the relevant window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# identifying context words for a narrow context window of 2 words on either side\n",
    "# of the target:\n",
    "# takes as input a sequence of words for counting. \n",
    "# For each word in the sequence, make 4 pairs:\n",
    "# (word, left neighbor of word), (word, left neighbor of left neighbor of word),\n",
    "# (word, right neighbor of word), (word, right neighbor of right neighbor of word),\n",
    "# so pair each word with all its context items in the context window.\n",
    "# Return a list of these pairs. \n",
    "def co_occurrences(wordsequence):\n",
    "    target_context_pairs = [ ]\n",
    "\n",
    "    # for a sequence of length N, count from 0 to N-1 \n",
    "    for index in range(len(wordsequence) - 1):\n",
    "        # count that word[index] as a target co-occurred with the next word as a context item,\n",
    "        # and vice versa\n",
    "        target_context_pairs.append( (wordsequence[index], wordsequence[index+1]) )\n",
    "        target_context_pairs.append( (wordsequence[index+1], wordsequence[index]) )\n",
    "\n",
    "        if index + 2 < len(wordsequence):\n",
    "            # there is a word 2 words away\n",
    "            # count that word[index] as a target co-occurred with the but-next word as a context item,\n",
    "            # and vice versa\n",
    "            target_context_pairs.append( (wordsequence[index], wordsequence[index+2]) )\n",
    "            target_context_pairs.append( (wordsequence[index+2], wordsequence[index]) )\n",
    "\n",
    "    return target_context_pairs\n",
    "\n",
    "###\n",
    "# run this to test co-occurrences\n",
    "def test_cooccurrences():\n",
    "    text = \"\"\"You will not find Dr. Jekyll; he is from home,\" replied Mr. Hyde\"\"\"\n",
    "    print(\"Testing the function that pairs up each target word with its context words.\")\n",
    "    print(\"Original text:\", text, \"\\n\")\n",
    "\n",
    "    words = preprocess(text)\n",
    "    cooc = co_occurrences(words)\n",
    "    print(\"These are the target/context pairs:\", cooc, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing the function that pairs up each target word with its context words.\n",
      "Original text: You will not find Dr. Jekyll; he is from home,\" replied Mr. Hyde \n",
      "\n",
      "These are the target/context pairs: [('you', 'will'), ('will', 'you'), ('you', 'not'), ('not', 'you'), ('will', 'not'), ('not', 'will'), ('will', 'find'), ('find', 'will'), ('not', 'find'), ('find', 'not'), ('not', 'dr'), ('dr', 'not'), ('find', 'dr'), ('dr', 'find'), ('find', 'jekyll'), ('jekyll', 'find'), ('dr', 'jekyll'), ('jekyll', 'dr'), ('dr', 'he'), ('he', 'dr'), ('jekyll', 'he'), ('he', 'jekyll'), ('jekyll', 'is'), ('is', 'jekyll'), ('he', 'is'), ('is', 'he'), ('he', 'from'), ('from', 'he'), ('is', 'from'), ('from', 'is'), ('is', 'home'), ('home', 'is'), ('from', 'home'), ('home', 'from'), ('from', 'replied'), ('replied', 'from'), ('home', 'replied'), ('replied', 'home'), ('home', 'mr'), ('mr', 'home'), ('replied', 'mr'), ('mr', 'replied'), ('replied', 'hyde'), ('hyde', 'replied'), ('mr', 'hyde'), ('hyde', 'mr')] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_cooccurrences()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Do the actual counting\n",
    "We store the counts in a table with one row for each target word, and one column for each context item. If our list of target words (which is also our list of context items) were \"apple\", \"grass\", \"truck\", this table could look like this: \n",
    "\n",
    "\n",
    "|    | apple\t| grass\t| truck |\n",
    "| --- | --- | --- |\n",
    "| apple\t| 0\t| 12 | 9 |\n",
    "| grass\t| 12 | 0 | 1 |\n",
    "| truck\t| 9\t| 1\t| 0 |\n",
    "\n",
    "\n",
    "It is symmetric, as \"apple\" as a target occurs with \"grass\" as a context item just as often as \"grass\" as a target occurs with \"apple\" as a context item. (In principle, we would only have to store half the matrix because it is symmetric, but we don't make use of that here to keep the code simple.)\n",
    "\n",
    "The numpy package has a data type \"array\" that fits our purposes nicely. See the tutorial for more info on numpy arrays. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "# We will need the function make_word_index below.\n",
    "# It maps each word that we want to keep around as a context item\n",
    "# to an index, which will be its place in the table of counts,\n",
    "# that is, its dimension in the space\n",
    "def make_word_index(keep_these_words):\n",
    "    # make an index that maps words from 'keep_these_words' to their index\n",
    "    word_index = { }\n",
    "    for index, word in enumerate(keep_these_words):\n",
    "        word_index[ word ] = index\n",
    "\n",
    "    return word_index\n",
    "\n",
    "import numpy\n",
    "\n",
    "# read all files in demo_dir, and compute a counts vector\n",
    "# of length numdims for each relevant word.\n",
    "# The function takes as input also a mapping word_index from relevant words\n",
    "# to their dimension, from which we derive a set relevant_words.\n",
    "# This function reads the texts one sentence at a time.\n",
    "# In each sentence, it identifies context words in the window\n",
    "# defined by co_occurrences(), and stores them if both the target\n",
    "# and its context words are relevant_words\n",
    "def make_space(demo_dir, word_index, numdims):\n",
    "\n",
    "    # relevant words: those that have an entry in word_index\n",
    "    relevant_words = set(word_index.keys())\n",
    "\n",
    "    # space: a mapping from relevant_words to an array of integers (raw counts)\n",
    "    space = { }\n",
    "    # fill the space with all zeros.\n",
    "    for word in relevant_words:\n",
    "        space[ word ] = numpy.zeros(numdims, dtype = numpy.int)\n",
    "\n",
    "    ##\n",
    "    # Design decision: We want to take sentence boundaries into account\n",
    "    # when computing distributional representations.\n",
    "    # So we need to detect sentence boundaries first.\n",
    "    sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "    # We iterate over the corpus files\n",
    "    # and count word co-occurrences in a window of 2\n",
    "    for filename in os.listdir(demo_dir):\n",
    "        if filename.endswith(\"txt\"):\n",
    "            #print(\"reading file\", filename)\n",
    "            # read the text\n",
    "            text = open(os.path.join(demo_dir, filename)).read()\n",
    "            # split the text into sentences\n",
    "            sentences = sent_detector.tokenize(text)\n",
    "            # process one sentence at a time\n",
    "            for sentence in sentences:\n",
    "                words = preprocess(sentence)\n",
    "\n",
    "                # determine pairs of co-occurrences to count,\n",
    "                # and store them in the matrix\n",
    "                for target, cxitem in co_occurrences(words):\n",
    "                    # are these two words relevant?\n",
    "                    if target in relevant_words and cxitem in relevant_words:\n",
    "                        # what is the row for this context item?\n",
    "                        cxitem_index = word_index[ cxitem]\n",
    "                        # now count\n",
    "                        space[ target ][cxitem_index] += 1\n",
    "\n",
    "\n",
    "    return space\n",
    "\n",
    "###\n",
    "# run this\n",
    "def test_space():\n",
    "    numdims = 50\n",
    "    # which words to use as targets and context words?\n",
    "    ktw = do_word_count(demo_dir, numdims)\n",
    "    # mapping words to an index, which will be their column\n",
    "    # in the table of counts\n",
    "    wi = make_word_index(ktw)\n",
    "    words_in_order = sorted(wi.keys(), key=lambda w:wi[w])\n",
    "    \n",
    "    print(\"word index:\")\n",
    "    for word in words_in_order:\n",
    "        print(word, wi[word], end= \" \")\n",
    "    print(\"\\n\")\n",
    "\n",
    "    space = make_space(demo_dir, wi, numdims)\n",
    "    \n",
    "    print(\"some words from the space\")\n",
    "    for w in words_in_order[:10]:\n",
    "        print(w,  space[w], \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word index:\n",
      "the 0 and 1 of 2 i 3 to 4 a 5 my 6 in 7 was 8 that 9 with 10 me 11 he 12 you 13 had 14 it 15 but 16 his 17 as 18 for 19 which 20 not 21 this 22 on 23 by 24 be 25 from 26 at 27 have 28 is 29 when 30 or 31 were 32 her 33 him 34 your 35 all 36 if 37 an 38 so 39 one 40 will 41 she 42 no 43 they 44 been 45 are 46 could 47 more 48 we 49 \n",
      "\n",
      "some words from the space\n",
      "the [ 126  855 2432  299  581   28   23  533  247  260  221  118   80   53\n",
      "  140   78  134   20  141  166  176   65   20  265  206   41  198  161\n",
      "   42   74   59   71   71   18   43    2   61   22    3   14   38   32\n",
      "   22    5   25   27   29   26   27   27] \n",
      "\n",
      "and [855 146 387 402 304 282 360 205 195 129 171 174  96  88  54 111  32 155\n",
      "  86  68  34  46  74  57  91  51  41  66  27  45  53  10  68  95  62  55\n",
      "  56  31  36  40  33  28  28  22  37  24  39  19  45  24] \n",
      "\n",
      "of [2432  387   58  112   52  506  370   64   65  138   34   34   20   30\n",
      "   33   39   40  151   36   30   74   15  134   19   13   18   12   12\n",
      "    5   17   11   56   30   56   25   62   52    7   77   18   78   17\n",
      "    9   27    7    8   12    4   18   11] \n",
      "\n",
      "i [299 402 112  86 235  99 156  82 315 381  57  96  22  89 291  71 182  15\n",
      " 164  63 168 225  60  58  38  39  39  49 186  13 162  20  19  11  40  22\n",
      "  15  55  15  32  18  65   6  51   3  36   3 105  18   3] \n",
      "\n",
      "to [581 304  52 235 100 201 280  93 185  83  58 316  55 160  64 113  52 100\n",
      "  50  37  37  84  54  40  23 185  32  24  69  49   7  45  41  69 107  40\n",
      "  32   7  42  23  26   7  15  25  11  20  25  11  30  17] \n",
      "\n",
      "a [ 28 282 506  99 201  20  13 204 124  67 145  47  36  26  42  59  58  11\n",
      "  86  89  44  34  23  54  63  19  42  36  32  46  11  34  12   6  19   3\n",
      "   6   8   3  42  11   9   7   4  12  11  12   4  38   9] \n",
      "\n",
      "my [ 23 360 370 156 280  13  62 214  98  93  64  42  26  38  51  24  66   5\n",
      "  29  67  34  20  10  59  34  17  68  34  25  43  26  13  48   0   9   3\n",
      "  21  15   0   4   9  23   4   2   8  10  22  12   7   1] \n",
      "\n",
      "in [533 205  64  82  93 204 214   0  82  47  21  64  27  29  35  40  38  70\n",
      "  19  25  47  16  59   6   7  20  11  12   6  21   7  19  27  30  20  31\n",
      "  14   4  25  16  24   6   7  10   9  14  18  10  11   5] \n",
      "\n",
      "was [247 195  65 315 185 124  98  82   6 135  48  18 111   4   3 247 104  46\n",
      "  42  25  56  51  58  27  67   9  12  27   2   0  39   7   0  24   4   6\n",
      "  21   7  14  27  24   3  41  20   1   1   0   1  15   1] \n",
      "\n",
      "that [260 129 138 381  83  67  93  47 135  22  26  64 100  96  86  50  39  24\n",
      "  21  19  27  44  18  29  14  28  15  34  34  52  18   9  15  10  10  10\n",
      "  32  24  10  32  19  30  27  15  25   8   8  30   4  15] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_space()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do you need a sparse matrix?\n",
    "A sparse matrix is a data type that only stores the non-zero entries in a table. If we have a matrix with lots of 0's (which we do, for the counts), this will save a lot of memory.\n",
    "It is somewhat slower in processing than a normal (dense) matrix though.\n",
    "\n",
    "The scipy package has not one, but several data types for sparse matrices. While we count co-occurrences, the matrix is slowly filled. For this, the scipy.sparse.lil_matrix type can be used, or scipy.sparse.coo_matrix. In the lil_matrix, updating counts is not as fast as in the coo_matrix, but the format is more straightforward. If you wanted to use a lil_matrix, the only thing you would need to change is the initialization of the matrix: "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import scipy\n",
    "\n",
    "# make a completely empty matrix\n",
    "# in which we want to store integers (that's the dtype)\n",
    "space = scipy.sparse.lil_matrix((5, 5), dtype = numpy.int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the counts in the matrix can be updated in the same way as in the numpy array above. For the coo_matrix, adding counts is less straightforward. You find a description of that matrix at https://scipy-lectures.github.io/advanced/scipy_sparse/coo_matrix.html . \n",
    "\n",
    "After all the counts have been recorded in the sparse matrix, you should convert it to a different scipy sparse matrix data type for doing math, as csr_matrix is much faster for that:\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "space = space.tocsr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the [documentation](http://docs.scipy.org/doc/scipy/reference/sparse.html), the [scipy tutorial](https://docs.scipy.org/doc/scipy/reference/tutorial/), and these [lecture notes](https://scipy-lectures.github.io/advanced/scipy_sparse/index.html) for more information on scipy sparse matrices. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Transform counts to association weights\n",
    "\n",
    "All words will have high co-occurrence counts with the most frequent context items. In our demo dataset, these are i-PR, the-DT, man-NN, on-CD, could-MD. This will falsely inflate all our similarity estimates. What we want to know instead is how strongly a target word is associated with a context item: Does it appear with the context item more often than we could expect at random? Less often? About as often as we would expect?\n",
    "\n",
    "There are multiple options for computing degree of association:\n",
    "\n",
    "- tf-idf (term frequency / inverse document frequency)\n",
    "- pointwise mutual information (PMI)\n",
    "- positive mutual information (PPMI): just change negative PMI values to zero\n",
    "- local mutual information (LMI)\n",
    "\n",
    "We do PPMI here. The PMI of a target word t and context item c is defined as:\n",
    "\n",
    "$$PMI(t,c) = \\log\\left[\\frac{p(t,c)}{p(t)p(c)}\\right]$$\n",
    "\n",
    "All the probabilities are computed from the table of counts. We need:\n",
    "\n",
    "- #(t, c): the co-occurrence count of t with c\n",
    "- #(_, _): the sum of counts in the whole table, across all targets\n",
    "- #(t, _): the sum of counts in the row of target t\n",
    "- #(_, c): the sum of counts in the column of context item c\n",
    "\n",
    "\n",
    "Then we have: \n",
    "\n",
    "- P(t, c) = #(t, c) / #(_, _)\n",
    "- P(t) = #(t, _) / #(_, _)\n",
    "- P(c) = #(_, c) / #(_, _)\n",
    "\n",
    "\n",
    "Here is the code for computing PPMI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "# transform the space using positive pointwise mutual information\n",
    "\n",
    "# target t, dimension value c, then\n",
    "# PMI(t, c) = log ( P(t, c) / (P(t) P(c)) )\n",
    "# where\n",
    "# P(t, c) = #(t, c) / #(_, _)\n",
    "# P(t) = #(t, _) / #(_, _)\n",
    "# P(c) = #(_, c) / #(_, _)\n",
    "#\n",
    "# PPMI(t, c) =   PMI(t, c) if PMI(t, c) > 0\n",
    "#                0 else\n",
    "def ppmi_transform(space, word_index):\n",
    "    # #(t, _): for each target word, sum up all its counts.\n",
    "    # row_sums is a dictionary mapping from target words to row sums\n",
    "    row_sums = { }\n",
    "    for word in space.keys():\n",
    "        row_sums[word] = space[word].sum()\n",
    "\n",
    "    # #(_, c): for each context word, sum up all its counts\n",
    "    # This should be the same as #(t, _) because the set of targets\n",
    "    # is the same as the set of contexts.\n",
    "    # col_sums is a dictionary mapping from context word indices to column sums\n",
    "    col_sums = { }\n",
    "    for index in word_index.values():\n",
    "        col_sums[ index ] = sum( [ vector[ index ] for vector in space.values() ])\n",
    "\n",
    "    # sanity check: row sums same as column sums?\n",
    "    for word in space.keys():\n",
    "        if row_sums[word] != col_sums[ word_index[word]]:\n",
    "            print(\"whoops, failed sanity check for\", word, row_sums[word], col_sums[word_index[word]])\n",
    "    \n",
    "    # #(_, _): overall count of occurrences. sum of all row_sums\n",
    "    all_sums = sum(row_sums.values())\n",
    "\n",
    "    # if all_sums is zero, there's nothing we can do\n",
    "    # because we then cannot divide by #(_, _)\n",
    "    if all_sums == 0:\n",
    "        print(\"completely empty space, returning it unchanged\")\n",
    "        return space\n",
    "\n",
    "    # P(t) = #(t, _) / #(_, _)\n",
    "    p_t = { }\n",
    "    for word in space.keys():\n",
    "        p_t[ word ] = row_sums[ word ] / all_sums\n",
    "\n",
    "    # P(c) = #(_, c) / #(_, _)\n",
    "    p_c = { }\n",
    "    for index in col_sums.keys():\n",
    "        p_c[ index ] = col_sums[ index ] / all_sums\n",
    "\n",
    "    # ppmi_space: a mapping from words to vectors of values \n",
    "    ppmi_space = { }\n",
    "    # first we map from words to values P(t, c)\n",
    "    for word in space.keys():\n",
    "        ppmi_space[ word ] = space[ word ] / all_sums\n",
    "    # divide each entry by P(t)\n",
    "    for word in space.keys():\n",
    "        if p_t[ word ] == 0:\n",
    "            # I haven't seen this word ever, so I cannot\n",
    "            # divide by P(t). But the whole entry for this word\n",
    "            # should be 0's, so leave as is.\n",
    "            pass\n",
    "        else:\n",
    "            ppmi_space[ word ] = ppmi_space[ word ] / p_t[ word ]\n",
    "    # divide each entry by P(c)\n",
    "    for index in p_c.keys():\n",
    "        if p_c[ index ] == 0:\n",
    "            # I haven't seen this context item ever,\n",
    "            # so I cannot divide by P(c).\n",
    "            # But every target word will have an entry of 0.0\n",
    "            # on this column, so nothing more to do.\n",
    "            pass\n",
    "        else:\n",
    "            for word in space.keys():\n",
    "                ppmi_space[ word ][index] = ppmi_space[ word][index] / p_c[ index ]\n",
    "                \n",
    "    # take the logarithm, ignore entries that are zero\n",
    "    for word in space.keys():\n",
    "        with numpy.errstate(divide=\"ignore\",invalid=\"ignore\"):\n",
    "            ppmi_space[ word ] = numpy.log(ppmi_space[ word ])\n",
    "            \n",
    "\n",
    "    # turn negative numbers to zero\n",
    "    for word in space.keys():\n",
    "        ppmi_space[word] = numpy.maximum(ppmi_space[word], 0.0)\n",
    "\n",
    "    return ppmi_space\n",
    "\n",
    "###\n",
    "# run this:\n",
    "def test_ppmispace():\n",
    "    numdims = 50\n",
    "    # which words to use as targets and context words?\n",
    "    ktw = do_word_count(demo_dir, numdims)\n",
    "    # mapping words to an index, which will be their column\n",
    "    # in the table of counts\n",
    "    wi = make_word_index(ktw)\n",
    "    words_in_order = sorted(wi.keys(), key=lambda w:wi[w])\n",
    "    \n",
    "    print(\"word index:\")\n",
    "    for word in words_in_order:\n",
    "        print(word, wi[word], end=\" \")\n",
    "    print(\"\\n\")\n",
    "\n",
    "    space = make_space(demo_dir, wi, numdims)\n",
    "    ppmispace = ppmi_transform(space, wi)\n",
    "    \n",
    "    print(\"some raw counts vectors and some ppmi vectors\")\n",
    "    for w in words_in_order[:10]:\n",
    "        print(\"---------\", \"\\n\", w)\n",
    "        print(\"raw\", space[w])\n",
    "        # for the PPMI space, we're rounding to 2 digits after the floating point\n",
    "        print(\"ppmi\", numpy.round(ppmispace[w], 2), \"\\n\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7: Dimensionality reduction\n",
    "Dimensionality reduction is a method that does exactly this: It takes a space where each word has a vector of, say, 10,000 dimensions and reduces it to a space where each word has a vector of something like 300 or 500 dimensions, making the space more manageable.\n",
    "\n",
    "The new dimensions can be seen as groupings (soft clusterings) of the old dimensions, or as latent semantic classes underlying the old dimensions. A popular choice of dimensionality reduction method is singular value decomposition (SVD). SVD involves representing a set of points in a different space (that is, through a new set of dimensions) in such a way that it brings out the underlying structure of the data.\n",
    "\n",
    "Here is how we can do this in Python. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################\n",
    "# transforming the space using singular value decomposition.\n",
    "# \n",
    "def svd_transform(space, originalnumdimensions,keepnumdimensions):\n",
    "    # space is a dictionary mapping words to vectors.\n",
    "    # combine those into a big matrix.\n",
    "    spacematrix = numpy.empty((len(space.keys()), originalnumdimensions))\n",
    "\n",
    "    rowlabels = sorted(space.keys())\n",
    "\n",
    "    for index, word in enumerate(rowlabels):\n",
    "        spacematrix[index] = space[word]\n",
    "\n",
    "    # now do SVD\n",
    "    umatrix, sigmavector, vmatrix = numpy.linalg.svd(spacematrix)\n",
    "\n",
    "    # remove the last few dimensions of u and sigma\n",
    "    utrunc = umatrix[:, :keepnumdimensions]\n",
    "    sigmatrunc = sigmavector[ :keepnumdimensions]\n",
    "\n",
    "    # new space: U %matrixproduct% Sigma_as_diagonal_matrix   \n",
    "    newspacematrix = numpy.dot(utrunc, numpy.diag(sigmatrunc))\n",
    "\n",
    "    # transform back to a dictionary mapping words to vectors\n",
    "    newspace = { }\n",
    "    for index, word in enumerate(rowlabels):\n",
    "        newspace[ word ] = newspacematrix[index]\n",
    "        \n",
    "    return newspace\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "####\n",
    "### run this:\n",
    "def test_svdspace():\n",
    "    numdims = 50\n",
    "    # which words to use as targets and context words?\n",
    "    ktw = do_word_count(demo_dir, numdims)\n",
    "    # mapping words to an index, which will be their column\n",
    "    # in the table of counts\n",
    "    wi = make_word_index(ktw)\n",
    "    words_in_order = sorted(wi.keys(), key=lambda w:wi[w])\n",
    "    \n",
    "    print(\"word index:\")\n",
    "    for word in words_in_order:\n",
    "        print(word, wi[word], end=\" \")\n",
    "    print(\"\\n\")\n",
    "\n",
    "    space = make_space(demo_dir, wi, numdims)\n",
    "    ppmispace = ppmi_transform(space, wi)\n",
    "    svdspace = svd_transform(ppmispace, numdims, 5)\n",
    "    \n",
    "    print(\"some vectors\")\n",
    "    for w in words_in_order[:10]:\n",
    "        print(\"--------------\", \"\\n\", w)\n",
    "        print(\"raw\", space[w])\n",
    "        # for the PPMI and SVD spaces, we're rounding to 2 digits after the floating point\n",
    "        print(\"ppmi\", numpy.round(ppmispace[w], 2), \"\\n\")\n",
    "        print(\"svd\", numpy.round(svdspace[w], 2), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word index:\n",
      "the 0 and 1 of 2 i 3 to 4 a 5 my 6 in 7 was 8 that 9 with 10 me 11 he 12 you 13 had 14 it 15 but 16 his 17 as 18 for 19 which 20 not 21 this 22 on 23 by 24 be 25 from 26 at 27 have 28 is 29 when 30 or 31 were 32 her 33 him 34 your 35 all 36 if 37 an 38 so 39 one 40 will 41 she 42 no 43 they 44 been 45 are 46 could 47 more 48 we 49 \n",
      "\n",
      "some vectors\n",
      "-------------- \n",
      " the\n",
      "raw [ 126  855 2432  299  581   28   23  533  247  260  221  118   80   53\n",
      "  140   78  134   20  141  166  176   65   20  265  206   41  198  161\n",
      "   42   74   59   71   71   18   43    2   61   22    3   14   38   32\n",
      "   22    5   25   27   29   26   27   27]\n",
      "ppmi [0.   0.31 1.37 0.   0.19 0.   0.   0.69 0.   0.   0.3  0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.35 0.23 0.   0.   0.83 0.71 0.   0.85 0.65\n",
      " 0.   0.   0.   0.24 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.  ] \n",
      "\n",
      "svd [-0.51 -0.59  1.16  1.2   0.45] \n",
      "\n",
      "-------------- \n",
      " and\n",
      "raw [855 146 387 402 304 282 360 205 195 129 171 174  96  88  54 111  32 155\n",
      "  86  68  34  46  74  57  91  51  41  66  27  45  53  10  68  95  62  55\n",
      "  56  31  36  40  33  28  28  22  37  24  39  19  45  24]\n",
      "ppmi [0.31 0.   0.   0.11 0.   0.3  0.59 0.13 0.   0.   0.45 0.36 0.   0.\n",
      " 0.   0.   0.   0.66 0.   0.   0.   0.   0.07 0.   0.29 0.   0.   0.16\n",
      " 0.   0.   0.02 0.   0.32 0.84 0.33 0.54 0.28 0.   0.1  0.09 0.   0.\n",
      " 0.   0.   0.22 0.   0.23 0.   0.5  0.04] \n",
      "\n",
      "svd [-0.71  0.32  0.76 -0.49  0.54] \n",
      "\n",
      "-------------- \n",
      " of\n",
      "raw [2432  387   58  112   52  506  370   64   65  138   34   34   20   30\n",
      "   33   39   40  151   36   30   74   15  134   19   13   18   12   12\n",
      "    5   17   11   56   30   56   25   62   52    7   77   18   78   17\n",
      "    9   27    7    8   12    4   18   11]\n",
      "ppmi [1.37 0.   0.   0.   0.   0.9  0.64 0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.65 0.   0.   0.   0.   0.68 0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.42 0.   0.33 0.   0.68 0.22 0.   0.88 0.   0.71 0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.  ] \n",
      "\n",
      "svd [-0.64  0.7   1.29 -1.24  0.47] \n",
      "\n",
      "-------------- \n",
      " i\n",
      "raw [299 402 112  86 235  99 156  82 315 381  57  96  22  89 291  71 182  15\n",
      " 164  63 168 225  60  58  38  39  39  49 186  13 162  20  19  11  40  22\n",
      "  15  55  15  32  18  65   6  51   3  36   3 105  18   3]\n",
      "ppmi [0.   0.11 0.   0.   0.   0.   0.   0.   0.62 0.86 0.   0.   0.   0.\n",
      " 1.01 0.   0.66 0.   0.65 0.   0.75 1.05 0.02 0.   0.   0.   0.   0.03\n",
      " 1.17 0.   1.3  0.   0.   0.   0.06 0.   0.   0.39 0.   0.03 0.   0.64\n",
      " 0.   0.59 0.   0.08 0.   1.25 0.   0.  ] \n",
      "\n",
      "svd [-1.9  -1.2  -0.66 -0.54 -1.14] \n",
      "\n",
      "-------------- \n",
      " to\n",
      "raw [581 304  52 235 100 201 280  93 185  83  58 316  55 160  64 113  52 100\n",
      "  50  37  37  84  54  40  23 185  32  24  69  49   7  45  41  69 107  40\n",
      "  32   7  42  23  26   7  15  25  11  20  25  11  30  17]\n",
      "ppmi [0.19 0.   0.   0.   0.   0.23 0.61 0.   0.19 0.   0.   1.23 0.   0.64\n",
      " 0.   0.19 0.   0.48 0.   0.   0.   0.17 0.02 0.   0.   1.35 0.   0.\n",
      " 0.29 0.02 0.   0.45 0.08 0.79 1.15 0.49 0.   0.   0.52 0.   0.   0.\n",
      " 0.   0.   0.   0.   0.05 0.   0.36 0.  ] \n",
      "\n",
      "svd [-1.06  0.48  0.72 -1.1   0.52] \n",
      "\n",
      "-------------- \n",
      " a\n",
      "raw [ 28 282 506  99 201  20  13 204 124  67 145  47  36  26  42  59  58  11\n",
      "  86  89  44  34  23  54  63  19  42  36  32  46  11  34  12   6  19   3\n",
      "   6   8   3  42  11   9   7   4  12  11  12   4  38   9]\n",
      "ppmi [0.   0.3  0.9  0.   0.23 0.   0.   0.83 0.22 0.   0.98 0.   0.   0.\n",
      " 0.   0.   0.05 0.   0.54 0.83 0.   0.   0.   0.34 0.63 0.   0.4  0.26\n",
      " 0.   0.39 0.   0.6  0.   0.   0.   0.   0.   0.   0.   0.84 0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   1.03 0.  ] \n",
      "\n",
      "svd [-0.92 -0.63  1.21  1.28  0.37] \n",
      "\n",
      "-------------- \n",
      " my\n",
      "raw [ 23 360 370 156 280  13  62 214  98  93  64  42  26  38  51  24  66   5\n",
      "  29  67  34  20  10  59  34  17  68  34  25  43  26  13  48   0   9   3\n",
      "  21  15   0   4   9  23   4   2   8  10  22  12   7   1]\n",
      "ppmi [0.   0.59 0.64 0.   0.61 0.   0.   0.92 0.04 0.04 0.21 0.   0.   0.\n",
      " 0.   0.   0.23 0.   0.   0.6  0.   0.   0.   0.48 0.06 0.   0.93 0.25\n",
      " 0.   0.37 0.05 0.   0.72 0.   0.   0.   0.05 0.   0.   0.   0.   0.19\n",
      " 0.   0.   0.   0.   0.4  0.   0.   0.  ] \n",
      "\n",
      "svd [-0.74 -0.92  0.79  0.84  0.69] \n",
      "\n",
      "-------------- \n",
      " in\n",
      "raw [533 205  64  82  93 204 214   0  82  47  21  64  27  29  35  40  38  70\n",
      "  19  25  47  16  59   6   7  20  11  12   6  21   7  19  27  30  20  31\n",
      "  14   4  25  16  24   6   7  10   9  14  18  10  11   5]\n",
      "ppmi [0.69 0.13 0.   0.   0.   0.83 0.92 0.   0.   0.   0.   0.21 0.   0.\n",
      " 0.   0.   0.   0.71 0.   0.   0.17 0.   0.7  0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.17 0.25 0.54 0.06 0.82 0.   0.   0.58 0.02 0.36 0.\n",
      " 0.   0.   0.   0.   0.31 0.   0.   0.  ] \n",
      "\n",
      "svd [-0.7   0.51  1.05 -1.2   0.63] \n",
      "\n",
      "-------------- \n",
      " was\n",
      "raw [247 195  65 315 185 124  98  82   6 135  48  18 111   4   3 247 104  46\n",
      "  42  25  56  51  58  27  67   9  12  27   2   0  39   7   0  24   4   6\n",
      "  21   7  14  27  24   3  41  20   1   1   0   1  15   1]\n",
      "ppmi [0.   0.   0.   0.62 0.19 0.22 0.04 0.   0.   0.41 0.   0.   0.79 0.\n",
      " 0.   1.45 0.68 0.19 0.   0.   0.24 0.16 0.58 0.   0.73 0.   0.   0.02\n",
      " 0.   0.   0.46 0.   0.   0.22 0.   0.   0.05 0.   0.   0.44 0.26 0.\n",
      " 1.11 0.24 0.   0.   0.   0.   0.15 0.  ] \n",
      "\n",
      "svd [-1.26  0.42  0.43  0.08 -0.62] \n",
      "\n",
      "-------------- \n",
      " that\n",
      "raw [260 129 138 381  83  67  93  47 135  22  26  64 100  96  86  50  39  24\n",
      "  21  19  27  44  18  29  14  28  15  34  34  52  18   9  15  10  10  10\n",
      "  32  24  10  32  19  30  27  15  25   8   8  30   4  15]\n",
      "ppmi [0.   0.   0.   0.86 0.   0.   0.04 0.   0.41 0.   0.   0.16 0.74 0.66\n",
      " 0.43 0.   0.   0.   0.   0.   0.   0.06 0.   0.   0.   0.   0.   0.3\n",
      " 0.12 0.62 0.   0.   0.   0.   0.   0.   0.52 0.2  0.   0.67 0.08 0.51\n",
      " 0.74 0.01 0.63 0.   0.   0.64 0.   0.37] \n",
      "\n",
      "svd [-1.47  0.56 -0.39  0.54 -0.38] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_svdspace()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 8: Computing similarity\n",
    "The main way in which distributional vectors are used is for estimating similarity between words. The central idea is that words that appear in similar contexts tend to be similar in meaning. So what we need to do to estimate word similarity from distributional vectors is to compute a similarity measure that determines how similar the vectors of two words are. There are many similarity measures, but the one that is used the most is cosine similarity. If we consider the vector of a word as an arrow from the origin, then two words should be similar if their vectors go in roughly the same direction. Cosine similarity measures this as the cosine of the angle between the two vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##\n",
    "# similarity measure: cosine\n",
    "#                           sum_i vec1_i * vec2_i\n",
    "# cosine(vec1, vec2) = ------------------------------\n",
    "#                        veclen(vec1) * veclen(vec2)\n",
    "# where\n",
    "#\n",
    "# veclen(vec) = squareroot( sum_i vec_i*vec_i )\n",
    "#\n",
    "\n",
    "import math\n",
    "\n",
    "def veclen(vector):\n",
    "    return math.sqrt(numpy.sum(numpy.square(vector)))\n",
    "\n",
    "def cosine(word1, word2, space):\n",
    "    vec1 = space[word1]\n",
    "    vec2 = space[word2]\n",
    "\n",
    "    veclen1 = veclen(vec1)\n",
    "    veclen2 = veclen(vec2)\n",
    "\n",
    "    if veclen1 == 0.0 or veclen2 == 0.0:\n",
    "        # one of the vectors is empty. make the cosine zero.\n",
    "        return 0.0\n",
    "\n",
    "    else:\n",
    "        # we could also simply do:\n",
    "        # dotproduct = numpy.dot(vec1, vec2)\n",
    "        dotproduct = numpy.sum(vec1 * vec2)\n",
    "\n",
    "        return dotproduct / (veclen1 * veclen2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######\n",
    "# run this:\n",
    "def test_cosine():\n",
    "    # this time we're not removing any words\n",
    "    numdims = 500\n",
    "    # which words to use as targets and context words?\n",
    "    ktw = do_word_count(demo_dir, numdims)\n",
    "    # mapping words to an index, which will be their column\n",
    "    # in the table of counts\n",
    "    wi = make_word_index(ktw)\n",
    "    words_in_order = sorted(wi.keys(), key=lambda w:wi[w])\n",
    "    \n",
    "    space = make_space(demo_dir, wi, numdims)\n",
    "    ppmispace = ppmi_transform(space, wi)\n",
    "    svdspace = svd_transform(ppmispace, numdims, 5)\n",
    "    \n",
    "    print(\"some cosines\")\n",
    "    print(\"'young' and 'little':\")\n",
    "    print(\"raw\", cosine(\"young\", \"little\", space))\n",
    "    print(\"ppmi\", cosine(\"young\", \"little\", ppmispace))\n",
    "    print(\"svd\", cosine(\"young\", \"little\", svdspace))\n",
    "\n",
    "    print(\"'a' and 'and':\")\n",
    "    print(\"raw\", cosine(\"a\", \"and\", space))\n",
    "    print(\"ppmi\", cosine(\"a\", \"and\", ppmispace))\n",
    "    print(\"svd\", cosine(\"a\", \"and\", svdspace))\n",
    "\n",
    "    print(\"'near' and 'far':\")\n",
    "    print(\"raw\", cosine(\"near\", \"far\", space))\n",
    "    print(\"ppmi\", cosine(\"near\", \"far\", ppmispace))\n",
    "    print(\"svd\", cosine(\"near\", \"far\", svdspace))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "some cosines\n",
      "'young' and 'little':\n",
      "raw 0.5890750308679625\n",
      "ppmi 0.15223709611006023\n",
      "svd 0.770512452846458\n",
      "'a' and 'and':\n",
      "raw 0.5803435173029969\n",
      "ppmi 0.23605987341863977\n",
      "svd 0.9500460172039358\n",
      "'near' and 'far':\n",
      "raw 0.48964841193158926\n",
      "ppmi 0.1458765714496384\n",
      "svd 0.9009450101543838\n"
     ]
    }
   ],
   "source": [
    "test_cosine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here is a function that uses cosine to compute the most similar word to a given target word. It simply computes its cosine similarity to all other words in the collection:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# finding the word most similar to a given target\n",
    "def most_similar_to(word1, space):\n",
    "\n",
    "    sims = [ (word2, cosine(word1, word2, space)) for word2 in space.keys() if word2 != word1 ]\n",
    "\n",
    "    return sorted(sims, key = lambda p:p[1], reverse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################\n",
    "# run this:\n",
    "def test_mostsimilar():\n",
    "    # this time we're not removing any words\n",
    "    numdims = 500\n",
    "    # which words to use as targets and context words?\n",
    "    ktw = do_word_count(demo_dir, numdims)\n",
    "    # mapping words to an index, which will be their column\n",
    "    # in the table of counts\n",
    "    wi = make_word_index(ktw)\n",
    "    words_in_order = sorted(wi.keys(), key=lambda w:wi[w])\n",
    "    \n",
    "    space = make_space(demo_dir, wi, numdims)\n",
    "    ppmispace = ppmi_transform(space, wi)\n",
    "    svdspace = svd_transform(ppmispace, numdims, 5)\n",
    "    \n",
    "    print(\"ten most similar to 'young':\")\n",
    "    print(\"raw\", most_similar_to(\"young\", space)[:10])\n",
    "    print(\"ppmi\", most_similar_to(\"young\", ppmispace)[:10])\n",
    "    print(\"svd\", most_similar_to(\"young\", svdspace)[:10])          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ten most similar to 'young':\n",
      "raw [('old', 0.8694114211270708), ('poor', 0.8568131600019072), ('beautiful', 0.8484279190885794), ('fire', 0.8440587903300643), ('figure', 0.8440167859391484), ('secret', 0.8398480413845959), ('person', 0.8394493793165739), ('second', 0.835548425116519), ('sound', 0.8337320943534208), ('letter', 0.8314104208355936)]\n",
      "ppmi [('old', 0.6119690903613226), ('whom', 0.5030698411469744), ('who', 0.5028266935093452), ('poor', 0.48491273874684965), ('good', 0.42076511309098136), ('whose', 0.3742322063201711), ('dead', 0.3661876627537445), ('dear', 0.34633791212686676), ('beautiful', 0.33824079961491105), ('little', 0.33623099854267857)]\n",
      "svd [('old', 0.9850600775742518), ('girl', 0.9787723485898417), ('lady', 0.9689192585981784), ('poor', 0.9683319940978361), ('gentleman', 0.9597664105449778), ('friend', 0.9496321408704241), ('man', 0.9477605028247725), ('whom', 0.938850774438674), ('woman', 0.9328929295900948), ('child', 0.9308932287532338)]\n"
     ]
    }
   ],
   "source": [
    "test_mostsimilar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
