{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Choose a corpus\n",
    "\n",
    "- More data is usually better because you get more stable estimates. But more data creates more processing headaches, both because of long processing times and large amounts of memory needed.\n",
    "- Genre matters! Dekang Lin, when computing a model only from newspaper data, got \"captive\" as the word that was overall most similar to \"westerner\", and vice versa. He also got \"Republican\" and \"terrorist\" as very close neighbors of \"adversary\" (though it is not clear whether they were found as synonyms or antonyms, as distributional models are not good at distinguishing those two.) \n",
    "\n",
    "For demonstration purposes, we choose a very short extract from Project Gutenberg, available [here](https://utexas.box.com/shared/static/aqwjnm50xj3wmn47mk3qkhr1yyn73loy.zip). If you would like to try a slightly bigger corpus, [try this version](https://utexas.box.com/shared/static/84ebvg8ajlbvgylr6bn0iwb8l0dj5uhd.zip). A much bigger collection of novels from Project Gutenberg is [here](https://utexas.box.com/shared/static/2n18p5cq98en2tcui3dahxhdc0n2h8qu.zip).\n",
    "\n",
    "Here is code that reads in one file of this collection at a time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading file pg42mini.txt\n"
     ]
    }
   ],
   "source": [
    "# Replace this directory with one on your own machine\n",
    "\n",
    "demo_dir = \"./gothic_novels_mini\"\n",
    "\n",
    "import os\n",
    "\n",
    "# We iterate over the corpus files. \n",
    "\n",
    "# os.listdir lists the names of all files in a directory\n",
    "for filename in os.listdir(demo_dir):\n",
    "    if filename.endswith(\"txt\"):\n",
    "        print(\"reading file\", filename)\n",
    "        text = open(os.path.join(demo_dir, filename)).read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Choose preprocessing steps\n",
    "- You probably want to lowercase all words.\n",
    "- Do you want to keep wordforms, or lemmatize them, or use a stemmer? Wordforms will have counts dispersed over different forms of the same word. Lemmatization is more accurate than stemming, but stemming is faster.\n",
    "- Do you want to apply part-of-speech tagging, for example to distinguish object-N and object-V? \n",
    "- Do you want to eliminate some words up front? Stopwords? Or maybe all words except a few class of content words, typically NN (nouns), JJ (adjectives), VB (verbs), RB (adverbs)?\n",
    "\n",
    "Here is code that splits a string into words and lowercases. You have done further preprocessing in homework 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this', 'is', 'a', 'test']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##\n",
    "# NLTK processing objects\n",
    "\n",
    "import nltk\n",
    "\n",
    "import string\n",
    "\n",
    "def preprocess(s):\n",
    "    # split up into words, lowercase, remove punctuation at beginning and end of word\n",
    "    return [ w.lower().strip(string.punctuation) for w in s.split() ]\n",
    "\n",
    "# or like this:\n",
    "# def preprocess(s):\n",
    "#     words =  [ ]\n",
    "#     for w in s.split():\n",
    "#         word = w.lower()\n",
    "#         word = word.strip(string.punctuation)\n",
    "#         words.append(word)\n",
    "#     return words\n",
    "\n",
    "\n",
    "# use the function like this:\n",
    "\n",
    "preprocess(\"This is a test.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Define a frequency cutoff\n",
    "Infrequent words are not useful for distributional models. An infrequent target word will have too low counts on all dimensions to get good similarity estimates. An infrequent context item will not contribute much to the representation of any targets. Also, the more different context items you count, the larger your table of counts. The table of counts can quickly become unwieldy, and using a frequency cutoff is a good way of keeping it manageable.\n",
    "\n",
    "We go through the whole corpus, and count the frequency of all words in it. Then we retain only the N most frequent words, for example N=2000 for our small demo corpus, or N=10,000 for a larger corpus. We use these words as both the list of target words and the list of context items. \n",
    "\n",
    "The following code does the counting and retains only the N most frequent words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################\n",
    "# Counting words:\n",
    "# We want to make a list of the N most frequent words in our corpus\n",
    "\n",
    "import os\n",
    "\n",
    "def do_word_count(demo_dir, numdims):\n",
    "    # we store the counts in word_count\n",
    "    # using NLTK's FreqDist\n",
    "    word_count = nltk.FreqDist()\n",
    "    \n",
    "    # We iterate over the corpus files\n",
    "    for filename in os.listdir(demo_dir):\n",
    "        if filename.endswith(\"txt\"):\n",
    "            print(\"reading file\", filename)\n",
    "            text = open(os.path.join(demo_dir, filename)).read()\n",
    "            word_count.update(preprocess(text))\n",
    "            \n",
    "    # keep_wordfreq is a list of (word, frequency) pairs\n",
    "    keep_wordfreq = word_count.most_common(numdims)\n",
    "    keep_these_words = [ w for w, freq in keep_wordfreq ]\n",
    "    # print(\"Target words:\\n\", keep_these_words, \"\\n\")\n",
    "    \n",
    "    return keep_these_words\n",
    "\n",
    "# or like this, without FreqDist:\n",
    "# def do_word_count(demo_dir, numdims):\n",
    "#     word_count = { }\n",
    "\n",
    "#     for filename in os.listdir(demo_dir):\n",
    "#         if filename.endswith(\"txt\"):\n",
    "#             print(\"reading file\", filename)\n",
    "#         text = open(os.path.join(demo_dir, filename)).read()\n",
    "#         for taggedword in preprocess(text):\n",
    "#             if taggedword not in word_count:\n",
    "#                 word_count[ taggedword ] = 0\n",
    "#             word_count[ taggedword ] += 1\n",
    "#\n",
    "#     def map_word_to_count(word): return word_count[ word ]\n",
    "#     keep_these_words = sorted(word_count.keys(), key = map_word_to_count)[:numdims]\n",
    "#     \n",
    "#     # print(\"Target words (and also dimensions):\\n\", keep_these_words, \"\\n\")\n",
    "#\n",
    "#     return keep_these_words\n",
    "\n",
    "\n",
    "\n",
    "##\n",
    "# run this:\n",
    "def test_wordcount():\n",
    "    print(\"Doing a frequency-based cutoff: keeping only the N most frequent context words.\")\n",
    "    \n",
    "    # with 10 dimensions\n",
    "    keepwords = do_word_count(demo_dir, 10)\n",
    "    print(\"Keeping only 10 dimensions, then I get:\", keepwords, \"\\n\")\n",
    "\n",
    "    # with 100 dimensions\n",
    "    keepwords = do_word_count(demo_dir, 100)\n",
    "    print(\"Keeping 100 dimensions, then I get:\", keepwords, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Choose a context window\n",
    "We want to make a table of counts for each target word. This table of counts will show how often each context item co-occurred with the target word. But what does it mean to co-occur? \n",
    "\n",
    "- A \"narrow context window\" is one that counts 2 or 3 words on either side of the target, or even all words in the sentence where the target occurs. If you count 2 or 3 words on either side of the target: Do you want to cross sentence boundaries?\n",
    "- A \"wide context window\" is one that counts 20 or 50 words on either side of the target, ignoring sentence boundaries, or maybe all words that occur in the same document as the target. (That only makes sense if you have lots of documents.)\n",
    "- If you have a corpus that has been analyzed for sentence structure by a syntactic parser, you can also say that your context window is all the parse tree snippets that link directly to your target word in the parse of the sentence. (But we don't do that here.)\n",
    "\n",
    "Here is a function that counts co-occurrences in a window of 2 words on either side of the target. It takes as input a sequence of words (supposed to be a single sentence), and returns a list of pairs (word1, word2) where word1 is a target and word2 is a context item that co-occurrence with the target in the relevant window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# identifying context words for a narrow context window of 2 words on either side\n",
    "# of the target:\n",
    "# takes as input a sequence of words for counting. \n",
    "# For each word in the sequence, make 4 pairs:\n",
    "# (word, left neighbor of word), (word, left neighbor of left neighbor of word),\n",
    "# (word, right neighbor of word), (word, right neighbor of right neighbor of word),\n",
    "# so pair each word with all its context items in the context window.\n",
    "# Return a list of these pairs. \n",
    "def co_occurrences(wordsequence):\n",
    "    target_context_pairs = [ ]\n",
    "\n",
    "    # for a sequence of length N, count from 0 to N-1 \n",
    "    for index in range(len(wordsequence) - 1):\n",
    "        # count that word[index] as a target co-occurred with the next word as a context item,\n",
    "        # and vice versa\n",
    "        target_context_pairs.append( (wordsequence[index], wordsequence[index+1]) )\n",
    "        target_context_pairs.append( (wordsequence[index+1], wordsequence[index]) )\n",
    "\n",
    "        if index + 2 < len(wordsequence):\n",
    "            # there is a word 2 words away\n",
    "            # count that word[index] as a target co-occurred with the but-next word as a context item,\n",
    "            # and vice versa\n",
    "            target_context_pairs.append( (wordsequence[index], wordsequence[index+2]) )\n",
    "            target_context_pairs.append( (wordsequence[index+2], wordsequence[index]) )\n",
    "\n",
    "    return target_context_pairs\n",
    "\n",
    "###\n",
    "# run this to test co-occurrences\n",
    "def test_cooccurrences():\n",
    "    text = \"\"\"You will not find Dr. Jekyll; he is from home,\" replied Mr. Hyde\"\"\"\n",
    "    print(\"Testing the function that pairs up each target word with its context words.\")\n",
    "    print(\"Original text:\", text, \"\\n\")\n",
    "\n",
    "    words = preprocess(text)\n",
    "    cooc = co_occurrences(words)\n",
    "    print(\"These are the target/context pairs:\", cooc, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Do the actual counting\n",
    "We store the counts in a table with one row for each target word, and one column for each context item. If our list of target words (which is also our list of context items) were \"apple\", \"grass\", \"truck\", this table could look like this: \n",
    "\n",
    "\n",
    "| apple\t| grass\t| truck |\n",
    "| --- | --- | --- |\n",
    "| apple\t| 0\t| 12 | 9 |\n",
    "| grass\t| 12 | 0 | 1 |\n",
    "| truck\t| 9\t| 1\t| 0 |\n",
    "\n",
    "\n",
    "It is symmetric, as \"apple\" as a target occurs with \"grass\" as a context item just as often as \"grass\" as a target occurs with \"apple\" as a context item. (In principle, we would only have to store half the matrix because it is symmetric, but we don't make use of that here to keep the code simple.)\n",
    "\n",
    "The numpy package has a data type \"array\" that fits our purposes nicely. See the tutorial for more info on numpy arrays. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "# We will need the function make_word_index below.\n",
    "# It maps each word that we want to keep around as a context item\n",
    "# to an index, which will be its place in the table of counts,\n",
    "# that is, its dimension in the space\n",
    "def make_word_index(keep_these_words):\n",
    "    # make an index that maps words from 'keep_these_words' to their index\n",
    "    word_index = { }\n",
    "    for index, word in enumerate(keep_these_words):\n",
    "        word_index[ word ] = index\n",
    "\n",
    "    return word_index\n",
    "\n",
    "import numpy\n",
    "\n",
    "# read all files in demo_dir, and compute a counts vector\n",
    "# of length numdims for each relevant word.\n",
    "# The function takes as input also a mapping word_index from relevant words\n",
    "# to their dimension, from which we derive a set relevant_words.\n",
    "# This function reads the texts one sentence at a time.\n",
    "# In each sentence, it identifies context words in the window\n",
    "# defined by co_occurrences(), and stores them if both the target\n",
    "# and its context words are relevant_words\n",
    "def make_space(demo_dir, word_index, numdims):\n",
    "\n",
    "    # relevant words: those that have an entry in word_index\n",
    "    relevant_words = set(word_index.keys())\n",
    "\n",
    "    # space: a mapping from relevant_words to an array of integers (raw counts)\n",
    "    space = { }\n",
    "    # fill the space with all zeros.\n",
    "    for word in relevant_words:\n",
    "        space[ word ] = numpy.zeros(numdims, dtype = numpy.int)\n",
    "\n",
    "    ##\n",
    "    # Design decision: We want to take sentence boundaries into account\n",
    "    # when computing distributional representations.\n",
    "    # So we need to detect sentence boundaries first.\n",
    "    sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "    # We iterate over the corpus files\n",
    "    # and count word co-occurrences in a window of 2\n",
    "    for filename in os.listdir(demo_dir):\n",
    "        if filename.endswith(\"txt\"):\n",
    "            print(\"reading file\", filename)\n",
    "            # read the text\n",
    "            text = open(os.path.join(demo_dir, filename)).read()\n",
    "            # split the text into sentences\n",
    "            sentences = sent_detector.tokenize(text)\n",
    "            # process one sentence at a time\n",
    "            for sentence in sentences:\n",
    "                words = preprocess(sentence)\n",
    "\n",
    "                # determine pairs of co-occurrences to count,\n",
    "                # and store them in the matrix\n",
    "                for target, cxitem in co_occurrences(words):\n",
    "                    # are these two words relevant?\n",
    "                    if target in relevant_words and cxitem in relevant_words:\n",
    "                        # what is the row for this context item?\n",
    "                        cxitem_index = word_index[ cxitem]\n",
    "                        # now count\n",
    "                        space[ target ][cxitem_index] += 1\n",
    "\n",
    "\n",
    "    return space\n",
    "\n",
    "###\n",
    "# run this\n",
    "def test_space():\n",
    "    numdims = 50\n",
    "    # which words to use as targets and context words?\n",
    "    ktw = do_word_count(demo_dir, numdims)\n",
    "    # mapping words to an index, which will be their column\n",
    "    # in the table of counts\n",
    "    wi = make_word_index(ktw)\n",
    "    words_in_order = sorted(wi.keys(), key=lambda w:wi[w])\n",
    "    \n",
    "    print(\"word index:\")\n",
    "    for word in words_in_order:\n",
    "        print(word, wi[word], end= \" \")\n",
    "    print(\"\\n\")\n",
    "\n",
    "    space = make_space(demo_dir, wi, numdims)\n",
    "    \n",
    "    print(\"some words from the space\")\n",
    "    for w in words_in_order[:10]:\n",
    "        print(w,  space[w], \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do you need a sparse matrix?\n",
    "A sparse matrix is a data type that only stores the non-zero entries in a table. If we have a matrix with lots of 0's (which we do, for the counts), this will save a lot of memory.\n",
    "It is somewhat slower in processing than a normal (dense) matrix though.\n",
    "\n",
    "The scipy package has not one, but several data types for sparse matrices. While we count co-occurrences, the matrix is slowly filled. For this, the scipy.sparse.lil_matrix type can be used, or scipy.sparse.coo_matrix. In the lil_matrix, updating counts is not as fast as in the coo_matrix, but the format is more straightforward. If you wanted to use a lil_matrix, the only thing you would need to change is the initialization of the matrix: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "\n",
    "# make a completely empty matrix\n",
    "# in which we want to store integers (that's the dtype)\n",
    "space = scipy.sparse.lil_matrix((5, 5), dtype = numpy.int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the counts in the matrix can be updated in the same way as in the numpy array above. For the coo_matrix, adding counts is less straightforward. You find a description of that matrix at https://scipy-lectures.github.io/advanced/scipy_sparse/coo_matrix.html . \n",
    "\n",
    "After all the counts have been recorded in the sparse matrix, you should convert it to a different scipy sparse matrix data type for doing math, as csr_matrix is much faster for that:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "space = space.tocsr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the [documentation](http://docs.scipy.org/doc/scipy/reference/sparse.html), the [scipy tutorial](https://docs.scipy.org/doc/scipy/reference/tutorial/), and these [lecture notes](https://scipy-lectures.github.io/advanced/scipy_sparse/index.html) for more information on scipy sparse matrices. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Transform counts to association weights\n",
    "\n",
    "All words will have high co-occurrence counts with the most frequent context items. In our demo dataset, these are i-PR, the-DT, man-NN, on-CD, could-MD. This will falsely inflate all our similarity estimates. What we want to know instead is how strongly a target word is associated with a context item: Does it appear with the context item more often than we could expect at random? Less often? About as often as we would expect?\n",
    "\n",
    "There are multiple options for computing degree of association:\n",
    "\n",
    "- tf-idf (term frequency / inverse document frequency)\n",
    "- pointwise mutual information (PMI)\n",
    "- positive mutual information (PPMI): just change negative PMI values to zero\n",
    "- local mutual information (LMI)\n",
    "\n",
    "We do PPMI here. The PMI of a target word t and context item c is defined as:\n",
    "\n",
    "$$PMI(t,c) = \\log\\left[\\frac{p(t,c)}{p(t)p(c)}\\right]$$\n",
    "\n",
    "All the probabilities are computed from the table of counts. We need:\n",
    "\n",
    "- #(t, c): the co-occurrence count of t with c\n",
    "- #(_, _): the sum of counts in the whole table, across all targets\n",
    "- #(t, _): the sum of counts in the row of target t\n",
    "- #(_, c): the sum of counts in the column of context item c\n",
    "\n",
    "\n",
    "Then we have: \n",
    "\n",
    "- P(t, c) = #(t, c) / #(_, _)\n",
    "- P(t) = #(t, _) / #(_, _)\n",
    "- P(c) = #(_, c) / #(_, _)\n",
    "\n",
    "\n",
    "Here is the code for computing PPMI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "# transform the space using positive pointwise mutual information\n",
    "\n",
    "# target t, dimension value c, then\n",
    "# PMI(t, c) = log ( P(t, c) / (P(t) P(c)) )\n",
    "# where\n",
    "# P(t, c) = #(t, c) / #(_, _)\n",
    "# P(t) = #(t, _) / #(_, _)\n",
    "# P(c) = #(_, c) / #(_, _)\n",
    "#\n",
    "# PPMI(t, c) =   PMI(t, c) if PMI(t, c) > 0\n",
    "#                0 else\n",
    "def ppmi_transform(space, word_index):\n",
    "    # #(t, _): for each target word, sum up all its counts.\n",
    "    # row_sums is a dictionary mapping from target words to row sums\n",
    "    row_sums = { }\n",
    "    for word in space.keys():\n",
    "        row_sums[word] = space[word].sum()\n",
    "\n",
    "    # #(_, c): for each context word, sum up all its counts\n",
    "    # This should be the same as #(t, _) because the set of targets\n",
    "    # is the same as the set of contexts.\n",
    "    # col_sums is a dictionary mapping from context word indices to column sums\n",
    "    col_sums = { }\n",
    "    for index in word_index.values():\n",
    "        col_sums[ index ] = sum( [ vector[ index ] for vector in space.values() ])\n",
    "\n",
    "    # sanity check: row sums same as column sums?\n",
    "    for word in space.keys():\n",
    "        if row_sums[word] != col_sums[ word_index[word]]:\n",
    "            print(\"whoops, failed sanity check for\", word, row_sums[word], col_sums[word_index[word]])\n",
    "    \n",
    "    # #(_, _): overall count of occurrences. sum of all row_sums\n",
    "    all_sums = sum(row_sums.values())\n",
    "\n",
    "    # if all_sums is zero, there's nothing we can do\n",
    "    # because we then cannot divide by #(_, _)\n",
    "    if all_sums == 0:\n",
    "        print(\"completely empty space, returning it unchanged\")\n",
    "        return space\n",
    "\n",
    "    # P(t) = #(t, _) / #(_, _)\n",
    "    p_t = { }\n",
    "    for word in space.keys():\n",
    "        p_t[ word ] = row_sums[ word ] / all_sums\n",
    "\n",
    "    # P(c) = #(_, c) / #(_, _)\n",
    "    p_c = { }\n",
    "    for index in col_sums.keys():\n",
    "        p_c[ index ] = col_sums[ index ] / all_sums\n",
    "\n",
    "    # ppmi_space: a mapping from words to vectors of values \n",
    "    ppmi_space = { }\n",
    "    # first we map from words to values P(t, c)\n",
    "    for word in space.keys():\n",
    "        ppmi_space[ word ] = space[ word ] / all_sums\n",
    "    # divide each entry by P(t)\n",
    "    for word in space.keys():\n",
    "        if p_t[ word ] == 0:\n",
    "            # I haven't seen this word ever, so I cannot\n",
    "            # divide by P(t). But the whole entry for this word\n",
    "            # should be 0's, so leave as is.\n",
    "            pass\n",
    "        else:\n",
    "            ppmi_space[ word ] = ppmi_space[ word ] / p_t[ word ]\n",
    "    # divide each entry by P(c)\n",
    "    for index in p_c.keys():\n",
    "        if p_c[ index ] == 0:\n",
    "            # I haven't seen this context item ever,\n",
    "            # so I cannot divide by P(c).\n",
    "            # But every target word will have an entry of 0.0\n",
    "            # on this column, so nothing more to do.\n",
    "            pass\n",
    "        else:\n",
    "            for word in space.keys():\n",
    "                ppmi_space[ word ][index] = ppmi_space[ word][index] / p_c[ index ]\n",
    "                \n",
    "    # take the logarithm, ignore entries that are zero\n",
    "    for word in space.keys():\n",
    "        with numpy.errstate(divide=\"ignore\",invalid=\"ignore\"):\n",
    "            ppmi_space[ word ] = numpy.log(ppmi_space[ word ])\n",
    "            \n",
    "\n",
    "    # turn negative numbers to zero\n",
    "    for word in space.keys():\n",
    "        ppmi_space[word] = numpy.maximum(ppmi_space[word], 0.0)\n",
    "\n",
    "    return ppmi_space\n",
    "\n",
    "###\n",
    "# run this:\n",
    "def test_ppmispace():\n",
    "    numdims = 50\n",
    "    # which words to use as targets and context words?\n",
    "    ktw = do_word_count(demo_dir, numdims)\n",
    "    # mapping words to an index, which will be their column\n",
    "    # in the table of counts\n",
    "    wi = make_word_index(ktw)\n",
    "    words_in_order = sorted(wi.keys(), key=lambda w:wi[w])\n",
    "    \n",
    "    print(\"word index:\")\n",
    "    for word in words_in_order:\n",
    "        print(word, wi[word], end=\" \")\n",
    "    print(\"\\n\")\n",
    "\n",
    "    space = make_space(demo_dir, wi, numdims)\n",
    "    ppmispace = ppmi_transform(space, wi)\n",
    "    \n",
    "    print(\"some raw counts vectors and some ppmi vectors\")\n",
    "    for w in words_in_order[:10]:\n",
    "        print(\"---------\", \"\\n\", w)\n",
    "        print(\"raw\", space[w])\n",
    "        # for the PPMI space, we're rounding to 2 digits after the floating point\n",
    "        print(\"ppmi\", numpy.round(ppmispace[w], 2), \"\\n\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7: Dimensionality reduction\n",
    "Dimensionality reduction is a method that does exactly this: It takes a space where each word has a vector of, say, 10,000 dimensions and reduces it to a space where each word has a vector of something like 300 or 500 dimensions, making the space more manageable.\n",
    "\n",
    "The new dimensions can be seen as groupings (soft clusterings) of the old dimensions, or as latent semantic classes underlying the old dimensions. A popular choice of dimensionality reduction method is singular value decomposition (SVD). SVD involves representing a set of points in a different space (that is, through a new set of dimensions) in such a way that it brings out the underlying structure of the data.\n",
    "\n",
    "Here is how we can do this in Python. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################\n",
    "# transforming the space using singular value decomposition.\n",
    "# \n",
    "def svd_transform(space, originalnumdimensions,keepnumdimensions):\n",
    "    # space is a dictionary mapping words to vectors.\n",
    "    # combine those into a big matrix.\n",
    "    spacematrix = numpy.empty((len(space.keys()), originalnumdimensions))\n",
    "\n",
    "    rowlabels = sorted(space.keys())\n",
    "\n",
    "    for index, word in enumerate(rowlabels):\n",
    "        spacematrix[index] = space[word]\n",
    "\n",
    "    # now do SVD\n",
    "    umatrix, sigmavector, vmatrix = numpy.linalg.svd(spacematrix)\n",
    "\n",
    "    # remove the last few dimensions of u and sigma\n",
    "    utrunc = umatrix[:, :keepnumdimensions]\n",
    "    sigmatrunc = sigmavector[ :keepnumdimensions]\n",
    "\n",
    "    # new space: U %matrixproduct% Sigma_as_diagonal_matrix   \n",
    "    newspacematrix = numpy.dot(utrunc, numpy.diag(sigmatrunc))\n",
    "\n",
    "    # transform back to a dictionary mapping words to vectors\n",
    "    newspace = { }\n",
    "    for index, word in enumerate(rowlabels):\n",
    "        newspace[ word ] = newspacematrix[index]\n",
    "        \n",
    "    return newspace\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "####\n",
    "### run this:\n",
    "def test_svdspace():\n",
    "    numdims = 50\n",
    "    # which words to use as targets and context words?\n",
    "    ktw = do_word_count(demo_dir, numdims)\n",
    "    # mapping words to an index, which will be their column\n",
    "    # in the table of counts\n",
    "    wi = make_word_index(ktw)\n",
    "    words_in_order = sorted(wi.keys(), key=lambda w:wi[w])\n",
    "    \n",
    "    print(\"word index:\")\n",
    "    for word in words_in_order:\n",
    "        print(word, wi[word], end=\" \")\n",
    "    print(\"\\n\")\n",
    "\n",
    "    space = make_space(demo_dir, wi, numdims)\n",
    "    ppmispace = ppmi_transform(space, wi)\n",
    "    svdspace = svd_transform(ppmispace, numdims, 5)\n",
    "    \n",
    "    print(\"some vectors\")\n",
    "    for w in words_in_order[:10]:\n",
    "        print(\"--------------\", \"\\n\", w)\n",
    "        print(\"raw\", space[w])\n",
    "        # for the PPMI and SVD spaces, we're rounding to 2 digits after the floating point\n",
    "        print(\"ppmi\", numpy.round(ppmispace[w], 2), \"\\n\")\n",
    "        print(\"svd\", numpy.round(svdspace[w], 2), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading file pg42mini.txt\n",
      "word index:\n",
      "the 0 and 1 in 2 his 3 was 4 a 5 of 6 never 7 something 8 which 9 but 10 mr 11 utterson 12 lawyer 13 man 14 rugged 15 countenance 16 that 17 lighted 18 by 19 smile 20 cold 21 scanty 22 embarrassed 23 discourse 24 backward 25 sentiment 26 lean 27 long 28 dusty 29 dreary 30 yet 31 somehow 32 lovable 33 at 34 friendly 35 meetings 36 when 37 wine 38 to 39 taste 40 eminently 41 human 42 beaconed 43 from 44 eye 45 indeed 46 found 47 its 48 way 49 \n",
      "\n",
      "reading file pg42mini.txt\n",
      "some vectors\n",
      "-------------- \n",
      " the\n",
      "raw [0 1 1 0 2 0 2 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      "ppmi [0.   0.53 0.8  0.   1.31 0.   1.85 0.   0.   0.   0.   2.41 2.   1.71\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   1.71 1.71 0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.  ] \n",
      "\n",
      "svd [-1.39  1.05  1.42 -1.56  1.07] \n",
      "\n",
      "-------------- \n",
      " and\n",
      "raw [1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 1 1 1 1 0 0 1 1\n",
      " 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "ppmi [0.53 0.   1.32 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   1.55 1.55 1.55 0.   0.   0.   0.\n",
      " 0.   1.55 1.55 1.55 1.83 0.   0.   1.83 1.55 1.55 0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.  ] \n",
      "\n",
      "svd [-1.29  3.28 -0.57 -1.1  -1.54] \n",
      "\n",
      "-------------- \n",
      " in\n",
      "raw [1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2 1 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "ppmi [0.8  1.32 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   1.81 2.5  2.5  1.81 1.81\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.  ] \n",
      "\n",
      "svd [-0.83  2.5  -0.98  0.96  1.45] \n",
      "\n",
      "-------------- \n",
      " his\n",
      "raw [0 0 0 0 1 0 1 0 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 1 1 0 0 1 1 1 0 0 0 1]\n",
      "ppmi [0.   0.   0.   0.   0.61 0.   1.15 0.   1.71 0.   2.41 0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.71 1.71 0.\n",
      " 0.   1.71 1.71 1.71 0.   0.   0.   2.  ] \n",
      "\n",
      "svd [-3.   -1.38 -1.2  -0.47  0.27] \n",
      "\n",
      "-------------- \n",
      " was\n",
      "raw [2 0 0 1 0 1 0 1 0 0 0 0 0 1 1 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 1 0 0 0 0 0 0 0 0 0 0]\n",
      "ppmi [1.31 0.   0.   0.61 0.   0.53 0.   0.93 0.   0.   0.   0.   0.   1.63\n",
      " 1.63 0.   1.63 1.63 1.63 0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.63 1.63 0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.  ] \n",
      "\n",
      "svd [-2.04  0.28  2.09 -0.51  0.9 ] \n",
      "\n",
      "-------------- \n",
      " a\n",
      "raw [0 0 0 0 1 0 2 0 0 0 0 0 0 1 2 1 1 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "ppmi [0.   0.   0.   0.   0.53 0.   1.76 0.   0.   0.   0.   0.   0.   1.63\n",
      " 2.32 1.63 1.63 0.   1.63 1.63 1.63 1.63 0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.  ] \n",
      "\n",
      "svd [-1.86  0.76  2.79 -0.24  0.93] \n",
      "\n",
      "-------------- \n",
      " of\n",
      "raw [2 0 0 1 0 2 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "ppmi [1.85 0.   0.   1.15 0.   1.76 0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 2.16 2.16 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.  ] \n",
      "\n",
      "svd [-1.33  0.29  1.47 -0.57  0.94] \n",
      "\n",
      "-------------- \n",
      " never\n",
      "raw [0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 1 1 0]\n",
      "ppmi [0.   0.   0.   0.   0.93 0.   0.   0.   0.   1.81 0.   0.   0.   0.\n",
      " 0.   0.   0.   2.03 2.03 2.03 0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   2.03 2.03 2.32 0.  ] \n",
      "\n",
      "svd [-2.54 -0.51  1.57  2.27 -1.35] \n",
      "\n",
      "-------------- \n",
      " something\n",
      "raw [0 0 0 2 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 1 1 0 0 1 1 0 0 0]\n",
      "ppmi [0.   0.   0.   1.71 0.   0.   0.   0.   0.   1.81 0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   2.03 2.03\n",
      " 2.03 0.   0.   2.03 2.03 0.   0.   0.  ] \n",
      "\n",
      "svd [-2.95 -1.6  -1.76 -0.34  0.09] \n",
      "\n",
      "-------------- \n",
      " which\n",
      "raw [0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 1 0 0]\n",
      "ppmi [0.   0.   0.   0.   0.   0.   0.   1.81 1.81 0.   3.19 0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   2.5  2.5  0.   0.  ] \n",
      "\n",
      "svd [-2.31 -1.01 -0.13  1.69 -1.33] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_svdspace()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 8: Computing similarity\n",
    "The main way in which distributional vectors are used is for estimating similarity between words. The central idea is that words that appear in similar contexts tend to be similar in meaning. So what we need to do to estimate word similarity from distributional vectors is to compute a similarity measure that determines how similar the vectors of two words are. There are many similarity measures, but the one that is used the most is cosine similarity. If we consider the vector of a word as an arrow from the origin, then two words should be similar if their vectors go in roughly the same direction. Cosine similarity measures this as the cosine of the angle between the two vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##\n",
    "# similarity measure: cosine\n",
    "#                           sum_i vec1_i * vec2_i\n",
    "# cosine(vec1, vec2) = ------------------------------\n",
    "#                        veclen(vec1) * veclen(vec2)\n",
    "# where\n",
    "#\n",
    "# veclen(vec) = squareroot( sum_i vec_i*vec_i )\n",
    "#\n",
    "\n",
    "import math\n",
    "\n",
    "def veclen(vector):\n",
    "    return math.sqrt(numpy.sum(numpy.square(vector)))\n",
    "\n",
    "def cosine(word1, word2, space):\n",
    "    vec1 = space[ word1 ]\n",
    "    vec2 = space[word2]\n",
    "\n",
    "    veclen1 = veclen(vec1)\n",
    "    veclen2 = veclen(vec2)\n",
    "\n",
    "    if veclen1 == 0.0 or veclen2 == 0.0:\n",
    "        # one of the vectors is empty. make the cosine zero.\n",
    "        return 0.0\n",
    "\n",
    "    else:\n",
    "        # we could also simply do:\n",
    "        # dotproduct = numpy.dot(vec1, vec2)\n",
    "        dotproduct = numpy.sum(vec1 * vec2)\n",
    "\n",
    "        return dotproduct / (veclen1 * veclen2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#######\n",
    "# run this:\n",
    "def test_cosine():\n",
    "    # this time we're not removing any words\n",
    "    numdims = 100\n",
    "    # which words to use as targets and context words?\n",
    "    ktw = do_word_count(demo_dir, numdims)\n",
    "    # mapping words to an index, which will be their column\n",
    "    # in the table of counts\n",
    "    wi = make_word_index(ktw)\n",
    "    words_in_order = sorted(wi.keys(), key=lambda w:wi[w])\n",
    "    \n",
    "    space = make_space(demo_dir, wi, numdims)\n",
    "    ppmispace = ppmi_transform(space, wi)\n",
    "    svdspace = svd_transform(ppmispace, numdims, 5)\n",
    "    \n",
    "    print(\"some cosines\")\n",
    "    print(\"'lawyer' and 'lean':\")\n",
    "    print(\"raw\", cosine(\"lawyer\", \"lean\", space))\n",
    "    print(\"ppmi\", cosine(\"lawyer\", \"lean\", ppmispace))\n",
    "    print(\"svd\", cosine(\"lawyer\", \"lean\", svdspace))\n",
    "    \n",
    "    print(\"'a' and 'and':\")\n",
    "    print(\"raw\", cosine(\"a\", \"and\", space))\n",
    "    print(\"ppmi\", cosine(\"a\", \"and\", ppmispace))\n",
    "    print(\"svd\", cosine(\"a\", \"and\", svdspace))\n",
    "\n",
    "    print(\"'friendly' and 'cold':\")\n",
    "    print(\"raw\", cosine(\"friendly\", \"cold\", space))\n",
    "    print(\"ppmi\", cosine(\"friendly\", \"cold\", ppmispace))\n",
    "    print(\"svd\", cosine(\"friendly\", \"cold\", svdspace))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading file pg42mini.txt\n",
      "reading file pg42mini.txt\n",
      "some cosines\n",
      "'lawyer' and 'lean':\n",
      "raw 0.0\n",
      "ppmi 0.0\n",
      "svd -0.21469089061564348\n",
      "'a' and 'and':\n",
      "raw 0.05892556509887897\n",
      "ppmi 0.0856440744498071\n",
      "svd 0.24545486879671974\n",
      "'friendly' and 'cold':\n",
      "raw 0.2886751345948129\n",
      "ppmi 0.11556202946694495\n",
      "svd 0.7237615847837177\n"
     ]
    }
   ],
   "source": [
    "test_cosine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here is a function that uses cosine to compute the most similar word to a given target word. It simply computes its cosine similarity to all other words in the collection:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# finding the word most similar to a given target\n",
    "def most_similar_to(word1, space):\n",
    "\n",
    "    sims = [ (word2, cosine(word1, word2, space)) for word2 in space.keys() if word2 != word1 ]\n",
    "\n",
    "    return sorted(sims, key = lambda p:p[1], reverse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################\n",
    "# run this:\n",
    "def test_mostsimilar():\n",
    "    # this time we're not removing any words\n",
    "    numdims = 100\n",
    "    # which words to use as targets and context words?\n",
    "    ktw = do_word_count(demo_dir, numdims)\n",
    "    # mapping words to an index, which will be their column\n",
    "    # in the table of counts\n",
    "    wi = make_word_index(ktw)\n",
    "    words_in_order = sorted(wi.keys(), key=lambda w:wi[w])\n",
    "    \n",
    "    space = make_space(demo_dir, wi, numdims)\n",
    "    ppmispace = ppmi_transform(space, wi)\n",
    "    svdspace = svd_transform(ppmispace, numdims, 5)\n",
    "    \n",
    "    print(\"ten most similar to 'friendly':\")\n",
    "    print(\"raw\", most_similar_to(\"friendly\", space)[:10])\n",
    "    print(\"ppmi\", most_similar_to(\"friendly\", ppmispace)[:10])\n",
    "    print(\"svd\", most_similar_to(\"friendly\", svdspace)[:10])          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading file pg42mini.txt\n",
      "reading file pg42mini.txt\n",
      "ten most similar to 'friendly':\n",
      "raw [('when', 0.5773502691896258), ('meetings', 0.5773502691896258), ('at', 0.40824829046386296), ('somehow', 0.33333333333333337), ('dusty', 0.2886751345948129), ('loudly', 0.2886751345948129), ('often', 0.2886751345948129), ('cold', 0.2886751345948129), ('yet', 0.2886751345948129), ('scanty', 0.2886751345948129)]\n",
      "ppmi [('meetings', 0.5380363087235843), ('when', 0.48910932448922206), ('at', 0.40661482187877174), ('and', 0.15903124309370048), ('loudly', 0.1382565678750535), ('somehow', 0.12177956745457398), ('embarrassed', 0.11802805576367716), ('cold', 0.11556202946694495), ('often', 0.11174472524035377), ('more', 0.11174472524035377)]\n",
      "svd [('at', 0.9955299182392863), ('meetings', 0.9947294939040962), ('and', 0.9565743446438643), ('somehow', 0.9306081064042174), ('yet', 0.9180504264497836), ('lovable', 0.9019436319798504), ('dreary', 0.8873343685210242), ('dusty', 0.8594912295154213), ('scanty', 0.8579260814713086), ('embarrassed', 0.8402160870207088)]\n"
     ]
    }
   ],
   "source": [
    "test_mostsimilar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
